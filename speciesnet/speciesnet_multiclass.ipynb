{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFTCwqrxGm_D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# program_prefix = \"/scratch1/lczhu/OP_CV/\"\n",
        "program_prefix = \"./\"\n",
        "program_id = \"species_net_test_multiclass_test\"\n",
        "num_epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi9vupvbFOBe",
        "outputId": "7e198ff7-94a4-460b-a569-b447bd4d329d"
      },
      "outputs": [],
      "source": [
        "image_class_labels_path = \"./image_class_labels_filtered.csv\"\n",
        "data = []\n",
        "\n",
        "with open(image_class_labels_path, newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        filepath = row[\"filepath\"]\n",
        "        class_id = int(row[\"class_id\"])\n",
        "        data.append((filepath, class_id))\n",
        "\n",
        "print(data[:10])\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YebF1-NvA-NQ",
        "outputId": "60dc5451-d3b5-4521-c531-292d078236f2"
      },
      "outputs": [],
      "source": [
        "# create and write column headers to csv files for logging purposes\n",
        "import csv\n",
        "log_folder_path = program_prefix + \"/log/\" + program_id + \"/\"\n",
        "os.makedirs(log_folder_path, exist_ok=True)\n",
        "\n",
        "train_csv_file_path = log_folder_path + \"train_scores.csv\"\n",
        "val_csv_file_path = log_folder_path + \"val_scores.csv\"\n",
        "time_csv_file_path = log_folder_path + \"time_per_epoch.csv\"\n",
        "test_csv_file_path = log_folder_path + \"test_scores.csv\"\n",
        "loss_csv_file_path = log_folder_path + \"loss_per_epoch.csv\"\n",
        "\n",
        "train_csv = open(train_csv_file_path, mode='w', newline='')\n",
        "val_csv = open(val_csv_file_path, mode='w', newline='')\n",
        "time_csv = open(time_csv_file_path, mode='w', newline='')\n",
        "test_csv = open(test_csv_file_path, mode='w', newline='')\n",
        "loss_csv = open(loss_csv_file_path, mode='w', newline='')\n",
        "\n",
        "train_writer = csv.writer(train_csv)\n",
        "val_writer = csv.writer(val_csv)\n",
        "time_writer = csv.writer(time_csv)\n",
        "test_writer = csv.writer(test_csv)\n",
        "loss_writer = csv.writer(loss_csv)\n",
        "\n",
        "train_writer.writerow([\n",
        "    'fold', 'epoch',\n",
        "    'accuracy_pangolin', 'accuracy_other',\n",
        "    'recall_pangolin', 'recall_other',\n",
        "    'precision_pangolin', 'precision_other',\n",
        "    'f1_pangolin', 'f1_other',\n",
        "    'auc_pangolin', 'auc_other'\n",
        "])\n",
        "val_writer.writerow([\n",
        "    'fold', 'epoch',\n",
        "    'accuracy_pangolin', 'accuracy_other',\n",
        "    'recall_pangolin', 'recall_other',\n",
        "    'precision_pangolin', 'precision_other',\n",
        "    'f1_pangolin', 'f1_other',\n",
        "    'auc_pangolin', 'auc_other'\n",
        "])\n",
        "time_writer.writerow(['fold', 'epoch', 'time'])\n",
        "test_writer.writerow([\n",
        "    'fold',\n",
        "    'accuracy_pangolin', 'accuracy_other',\n",
        "    'recall_pangolin', 'recall_other',\n",
        "    'precision_pangolin', 'precision_other',\n",
        "    'f1_pangolin', 'f1_other',\n",
        "    'auc_pangolin', 'auc_other'\n",
        "])\n",
        "loss_writer.writerow(['fold', 'epoch', 'train_loss', 'val_loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from speciesnet import SpeciesNet\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import time\n",
        "import random\n",
        "\n",
        "speciesnet_model_path = program_prefix + 'speciesnet_model'\n",
        "model_folder_path = program_prefix + \"models/\" + program_id + \"/\"                 # create folder to store optimal model per fold\n",
        "feature_map_folder_path = program_prefix + \"feature_maps/\" + program_id + \"/\"\n",
        "confusion_matrix_folder_path = program_prefix + \"cm/\" + program_id + \"/\"\n",
        "misclassified_folder_path = program_prefix + \"misclassified/\" + program_id + \"/\"\n",
        "os.makedirs(model_folder_path, exist_ok=True)\n",
        "os.makedirs(feature_map_folder_path, exist_ok=True)\n",
        "os.makedirs(confusion_matrix_folder_path, exist_ok=True)\n",
        "os.makedirs(misclassified_folder_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pangolin_indices = []\n",
        "with open(\"./speciesnet_model/always_crop_99710272_22x8_v12_epoch_00148.labels.txt\", \"r\") as f:\n",
        "    for idx, line in enumerate(f):\n",
        "        if \"pangolin\" in line.lower():\n",
        "            pangolin_indices.append(idx)\n",
        "\n",
        "# lines 882, 1405, 1406\n",
        "# 2f336cdf-a62f-4587-a516-6e6c74d07353;mammalia;pholidota;manidae;phataginus;tricuspis;white-bellied pangolin\n",
        "# b1cefdc9-af34-4f28-b077-1186dd6b5072;mammalia;pholidota;manidae;;;pangolin family\n",
        "# ade3ecab-c110-429a-849e-b6afdb290219;mammalia;pholidota;manidae;manis;;pangolin species\n",
        "\n",
        "# pangolin_indices = [1404] # family level\n",
        "# pangolin_indices = [1405] # species level\n",
        "\n",
        "print(pangolin_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNLWFI5Mc3YA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        ")\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from speciesnet.utils import BBox\n",
        "\n",
        "# plot image of misclassifications\n",
        "def plot_misclassified(fold, pred, data):\n",
        "    true = [label for _, label in data]\n",
        "    binary_true = np.isin(true, pangolin_indices).astype(int)\n",
        "    fn_indices = [i for i, (p, t) in enumerate(zip(pred, binary_true)) if p == 0 and t == 1]   # false negatives\n",
        "    fp_indices = [i for i, (p, t) in enumerate(zip(pred, binary_true)) if p == 1 and t == 0]   # false positives\n",
        "\n",
        "    # helper method to plot image\n",
        "    def plot_images(indices, title):\n",
        "        if not indices:\n",
        "            print(f\"No Misclassified Samples Found: \" + title)\n",
        "            return\n",
        "        \n",
        "        print(f\"Misclassified Samples Found: \" + str(len(indices)))\n",
        "        indices = indices[:5]\n",
        "        \n",
        "        fig, axes = plt.subplots(1, len(indices), figsize=(15, 5))\n",
        "        if len(indices) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for ax, idx in zip(axes, indices):\n",
        "            filename, true_label = data[idx]\n",
        "            img = Image.open(filename).convert(\"L\")            \n",
        "            ax.imshow(img, cmap=\"gray\")\n",
        "            ax.axis(\"off\")\n",
        "            ax.set_title(f\"{title}\\nPred: {pred[idx]}, True: {true_label}\", fontsize=10)\n",
        "            \n",
        "        misclassified_image_path  = misclassified_folder_path + \"misclassified_fold_\" + str(fold + 1) + \"/\"     # path to store the misclassified images, name-specific to fold \n",
        "        misclassified_image = misclassified_image_path + str(idx) + \".png\"\n",
        "\n",
        "        os.makedirs(misclassified_image_path, exist_ok=True)\n",
        "        plt.savefig(misclassified_image, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    # plot images\n",
        "    plot_images(fn_indices, \"Fold \" + str(fold + 1) + \"- False Negatives\")\n",
        "    plot_images(fp_indices, \"Fold \" + str(fold + 1) + \"- False Positives\")\n",
        "\n",
        "###############################################################################################################################################\n",
        "# printing/logging methods (different for validation, training, and testing because they have different ways to log to csv files)\n",
        "###############################################################################################################################################\n",
        "\n",
        "# prints score for validation\n",
        "def val_print_and_log_scores(fold, epoch, true, pred, prob):\n",
        "    # calculating scores for pangolin class\n",
        "    accuracy = accuracy_score(true, pred)\n",
        "    pangolin_precision = precision_score(true, pred, pos_label=1, zero_division=0)\n",
        "    pangolin_recall = recall_score(true, pred, pos_label=1, zero_division=0)\n",
        "    pangolin_f1 = f1_score(true, pred, pos_label=1, zero_division=0)\n",
        "    pangolin_prob = prob\n",
        "    pangolin_auc = roc_auc_score(true, pangolin_prob)\n",
        "\n",
        "    print(f\"\\tpangolin accuracy: {accuracy:.4f}\")\n",
        "    print(f\"\\tpangolin precision: {pangolin_precision:.4f}\")\n",
        "    print(f\"\\tpangolin recall: {pangolin_recall:.4f}\")\n",
        "    print(f\"\\tpangolin f1-score: {pangolin_f1:.4f}\")\n",
        "    print(f\"\\tpangolin auc: {pangolin_auc:.4f}\")\n",
        "\n",
        "    # calculating scores for other class\n",
        "    other_precision = precision_score(true, pred, pos_label=0, zero_division=0)\n",
        "    other_recall = recall_score(true, pred, pos_label=0, zero_division=0)\n",
        "    other_f1 = f1_score(true, pred, pos_label=0, zero_division=0)\n",
        "\n",
        "    other_prob = prob\n",
        "    true_inverted = [1 if t != 1 else 0 for t in true]\n",
        "    other_auc = roc_auc_score(true_inverted, other_prob)\n",
        "\n",
        "    print(f\"\\tother accuracy: {accuracy:.4f}\")\n",
        "    print(f\"\\tother precision: {other_precision:.4f}\")\n",
        "    print(f\"\\tother recall: {other_recall:.4f}\")\n",
        "    print(f\"\\tother f1-score: {other_f1:.4f}\")\n",
        "    print(f\"\\tother auc: {other_auc:.4f}\")\n",
        "\n",
        "    # writing to csv file\n",
        "    val_writer.writerow([\n",
        "            fold, epoch,\n",
        "            accuracy, accuracy,\n",
        "            pangolin_recall, other_recall,\n",
        "            pangolin_precision, other_precision,\n",
        "            pangolin_f1, other_f1,\n",
        "            pangolin_auc, other_auc\n",
        "    ])\n",
        "\n",
        "\n",
        "# print and log score for training\n",
        "def train_print_and_log_scores(fold, epoch, true, pred, prob):\n",
        "    # calculating scores for pangolin class\n",
        "    accuracy = accuracy_score(true, pred)\n",
        "    pangolin_precision = precision_score(true, pred, pos_label=1, zero_division=0)\n",
        "    pangolin_recall = recall_score(true, pred, pos_label=1, zero_division=0)\n",
        "    pangolin_f1 = f1_score(true, pred, pos_label=1, zero_division=0)\n",
        "    pangolin_prob = prob\n",
        "    pangolin_auc = roc_auc_score(true, pangolin_prob)\n",
        "\n",
        "    print(f\"\\tpangolin accuracy: {accuracy:.4f}\")\n",
        "    print(f\"\\tpangolin precision: {pangolin_precision:.4f}\")\n",
        "    print(f\"\\tpangolin recall: {pangolin_recall:.4f}\")\n",
        "    print(f\"\\tpangolin f1-score: {pangolin_f1:.4f}\")\n",
        "    print(f\"\\tpangolin auc: {pangolin_auc:.4f}\")\n",
        "\n",
        "    # calculating scores for other class\n",
        "    other_precision = precision_score(true, pred, pos_label=0, zero_division=0)\n",
        "    other_recall = recall_score(true, pred, pos_label=0, zero_division=0)\n",
        "    other_f1 = f1_score(true, pred, pos_label=0, zero_division=0)\n",
        "    other_prob = prob\n",
        "    true_inverted = (true != 1) \n",
        "    other_auc = roc_auc_score(true_inverted, other_prob)\n",
        "\n",
        "    print(f\"\\tother accuracy: {accuracy:.4f}\")\n",
        "    print(f\"\\tother precision: {other_precision:.4f}\")\n",
        "    print(f\"\\tother recall: {other_recall:.4f}\")\n",
        "    print(f\"\\tother f1-score: {other_f1:.4f}\")\n",
        "    print(f\"\\tother auc: {other_auc:.4f}\")\n",
        "\n",
        "    # writing to csv file\n",
        "    train_writer.writerow([\n",
        "        fold, epoch,\n",
        "        accuracy, accuracy,\n",
        "        pangolin_recall, other_recall,\n",
        "        pangolin_precision, other_precision,\n",
        "        pangolin_f1, other_f1,\n",
        "        pangolin_auc, other_auc\n",
        "    ])\n",
        "\n",
        "# print and log scores for testing\n",
        "def test_print_and_log_scores(fold, true, pred, prob):\n",
        "    # calculating scores for pangolin class\n",
        "    accuracy = accuracy_score(true, pred)\n",
        "    pangolin_precision = precision_score(true, pred, pos_label=1, zero_division=0)\n",
        "    pangolin_recall = recall_score(true, pred, pos_label=1, zero_division=0)\n",
        "    pangolin_f1 = f1_score(true, pred, pos_label=1, zero_division=0)\n",
        "    pangolin_prob = prob\n",
        "    pangolin_auc = roc_auc_score(true, pangolin_prob)\n",
        "\n",
        "    print(f\"\\tpangolin accuracy: {accuracy:.4f}\")\n",
        "    print(f\"\\tpangolin precision: {pangolin_precision:.4f}\")\n",
        "    print(f\"\\tpangolin recall: {pangolin_recall:.4f}\")\n",
        "    print(f\"\\tpangolin f1-score: {pangolin_f1:.4f}\")\n",
        "    print(f\"\\tpangolin auc: {pangolin_auc:.4f}\")\n",
        "\n",
        "    # calculating scores for other class\n",
        "    other_precision = precision_score(true, pred, pos_label=0, zero_division=0)\n",
        "    other_recall = recall_score(true, pred, pos_label=0, zero_division=0)\n",
        "    other_f1 = f1_score(true, pred, pos_label=0, zero_division=0)\n",
        "    other_prob = prob\n",
        "    true_inverted = [1 if t != 1 else 0 for t in true]\n",
        "    other_auc = roc_auc_score(true_inverted, other_prob)\n",
        "\n",
        "    print(f\"\\tother accuracy: {accuracy:.4f}\")\n",
        "    print(f\"\\tother precision: {other_precision:.4f}\")\n",
        "    print(f\"\\tother recall: {other_recall:.4f}\")\n",
        "    print(f\"\\tother f1-score: {other_f1:.4f}\")\n",
        "    print(f\"\\tother auc: {other_auc:.4f}\")\n",
        "\n",
        "    # writing to csv file\n",
        "    test_writer.writerow([\n",
        "        fold,\n",
        "        accuracy, accuracy,\n",
        "        pangolin_recall, other_recall,\n",
        "        pangolin_precision, other_precision,\n",
        "        pangolin_f1, other_f1,\n",
        "        pangolin_auc, other_auc\n",
        "    ])\n",
        "\n",
        "# custom collate function for the data loader\n",
        "def collate_fn(batch):\n",
        "    images = torch.stack([item[\"image\"] for item in batch])\n",
        "    labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
        "    image_ids = [item[\"image_id\"] for item in batch]\n",
        "    detections = [item[\"detections\"] for item in batch]\n",
        "\n",
        "    return {\n",
        "        \"image\": images,\n",
        "        \"label\": labels,\n",
        "        \"image_id\": image_ids,\n",
        "        \"detections\": detections\n",
        "    }\n",
        "\n",
        "# custom image dataset\n",
        "class ImagesDataset(Dataset):\n",
        "    def __init__(self, data, detector):\n",
        "        self.data = data\n",
        "        self.detector = detector\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((480, 480)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        filepath, label = self.data[index]\n",
        "        image = Image.open(filepath).convert(\"RGB\")\n",
        "\n",
        "        # preprocessing: crop out bottom bar and convert to greyscale\n",
        "        image = self._crop_bottom_bar(image)\n",
        "        image = image.convert(\"L\").convert(\"RGB\") # converts back to rgb to preserve 3 channels\n",
        "\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        # preprocessing with MD\n",
        "        image_MD = self.detector.preprocess(image)\n",
        "        detection_result = self.detector.predict(filepath, image_MD)\n",
        "        detections = detection_result[\"detections\"]\n",
        "\n",
        "        image_bbox = image\n",
        "        # use bounding box to crop image \n",
        "        if detections:\n",
        "            # get detection with highest confidence\n",
        "            bbox = BBox(*detections[0][\"bbox\"])\n",
        "\n",
        "            x0 = bbox.xmin * image.width\n",
        "            y0 = bbox.ymin * image.height\n",
        "            x1 = (bbox.xmin + bbox.width) * image.width\n",
        "            y1 = (bbox.ymin + bbox.height) * image.height\n",
        "\n",
        "            image_bbox = image.crop((x0, y0, x1, y1))\n",
        "\n",
        "            # extend cropped image for square dimensions\n",
        "            width = x1 - x0\n",
        "            height = y1 - y0\n",
        "            diff = abs(width - height)\n",
        "\n",
        "            if width > height:\n",
        "                y0 = max(0, y0 - diff // 2)\n",
        "                y1 = min(image.height, y1 + (diff - diff // 2))\n",
        "            elif height > width:\n",
        "                x0 = max(0, x0 - diff // 2)\n",
        "                x1 = min(image.width, x1 + (diff - diff // 2))\n",
        "\n",
        "            image_bbox = image.crop((x0, y0, x1, y1))\n",
        "\n",
        "        # ensure final image is square\n",
        "        image_bbox = self._pad_to_square(image_bbox)\n",
        "\n",
        "        # transform image\n",
        "        image = self.transform(image_bbox)\n",
        "\n",
        "        sample = {\"image\": image, \"label\": label, \"image_id\": index, \"detections\": detections}\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        \n",
        "    # crops out WI info bar at bottom of image \n",
        "    def _crop_bottom_bar(self, image):\n",
        "        width, height = image.size\n",
        "        if (width != 5376 and height != 3024): return image\n",
        "        bar_height = int(height * 0.05)\n",
        "        cropped_image = image.crop((0, 0, width, height - bar_height))\n",
        "        return cropped_image\n",
        "\n",
        "    # pads non square images with black pixels\n",
        "    def _pad_to_square(self, image):\n",
        "        width, height = image.size\n",
        "        if width == height:\n",
        "            return image\n",
        "\n",
        "        max_side = max(width, height)\n",
        "        image_padded = Image.new(\"RGB\", (max_side, max_side), color=(0, 0, 0))\n",
        "        x = (max_side - width) // 2\n",
        "        y = (max_side - height) // 2\n",
        "        image_padded.paste(image, (x, y))\n",
        "        return image_padded\n",
        "\n",
        "# store feature maps for each layer\n",
        "feature_maps = {}\n",
        "\n",
        "# hook function to save feature maps\n",
        "def hook_fn(module, input, output, name):\n",
        "    feature_maps[name] = output.detach()\n",
        "\n",
        "\n",
        "layers_to_visualize = [\n",
        "    'SpeciesNet/efficientnetv2-m/stem_conv/Conv2D.1',               # very early\n",
        "    'SpeciesNet/efficientnetv2-m/block2a_expand_conv/Conv2D',       # early mid\n",
        "    'SpeciesNet/efficientnetv2-m/block4a_project_conv/Conv2D',      # mid\n",
        "    'SpeciesNet/efficientnetv2-m/block6b_project_conv/Conv2D',      # late mid\n",
        "    'SpeciesNet/efficientnetv2-m/top_conv/Conv2D'                   # final conv\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read cross validation indices from stored csv file\n",
        "df_splits = pd.read_csv(\"v2-multiclass_splits.csv\")\n",
        "df_splits = df_splits.iloc[::100] ####### for testing purposes\n",
        "print(df_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_file = \"./speciesnet_model/always_crop_99710272_22x8_v12_epoch_00148.labels.txt\"\n",
        "\n",
        "with open(label_file, \"r\") as f:\n",
        "    class_names = [line.strip() for line in f]\n",
        "\n",
        "len(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_weights = torch.ones(len(class_names))\n",
        "\n",
        "for idx in pangolin_indices:\n",
        "    class_weights[idx] = 5.0\n",
        "\n",
        "class_weights = class_weights.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P5F1ZXhc6nu1",
        "outputId": "23ac6988-d09e-41ca-ca14-bcb563552033"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "for fold in df_splits[\"fold\"].unique():   \n",
        "    image_index = 0     # indices for feature maps\n",
        "     \n",
        "    # get the stored indices \n",
        "    train_index = df_splits[(df_splits[\"fold\"] == fold) & (df_splits[\"type\"] == \"train\")][\"index\"].values\n",
        "    val_index = df_splits[(df_splits[\"fold\"] == fold) & (df_splits[\"type\"] == \"val\")][\"index\"].values\n",
        "    test_index = df_splits[(df_splits[\"fold\"] == fold) & (df_splits[\"type\"] == \"test\")][\"index\"].values\n",
        "\n",
        "    print(f\"\\n\\nfold {fold + 1} -------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    min_val_loss = sys.float_info.max\n",
        "    model_name = program_prefix + \"model_fold\" + str(fold + 1) + \".pth\"\n",
        "    model_path = model_folder_path + \"/\" + model_name       # path to store the model checkpoints, name-specific to fold\n",
        "\n",
        "    # load model\n",
        "    speciesnet_model = SpeciesNet(speciesnet_model_path)\n",
        "    speciesnet_classifier = speciesnet_model.classifier\n",
        "    speciesnet_detector = speciesnet_model.detector\n",
        "    speciesnet_ensembler = speciesnet_model.ensemble\n",
        "\n",
        "    base_model = speciesnet_classifier.model\n",
        "    base_model.fc = nn.Sequential(\n",
        "        nn.Linear(2048, 100),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(100, 2498),  # output a single value for binary classification\n",
        "        # nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    model = base_model.to(device)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'block7' in name or 'top_conv' in name or 'fc' in name:\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # split data and get dataloaders\n",
        "    train_data = [data[i] for i in train_index]\n",
        "    val_data = [data[i] for i in val_index]\n",
        "    test_data = [data[i] for i in test_index]\n",
        "\n",
        "    train_dataset = ImagesDataset(train_data, speciesnet_detector)\n",
        "    val_dataset = ImagesDataset(val_data, speciesnet_detector)\n",
        "    test_dataset = ImagesDataset(test_data, speciesnet_detector)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "    # register hooks for each layer\n",
        "    hooks = []\n",
        "    for layer_name in layers_to_visualize:\n",
        "        layer = dict([*model.named_modules()])[layer_name]  # Get the layer by name\n",
        "        hook = layer.register_forward_hook(lambda module, input, output, name=layer_name: hook_fn(module, input, output, name))\n",
        "        hooks.append(hook)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"\\nepoch {epoch}\")\n",
        "\n",
        "        #############################################################################################################################################\n",
        "        # training\n",
        "        #############################################################################################################################################\n",
        "        print(\"TRAINING\")\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        tracking_loss = {}\n",
        "        training_loss = 0\n",
        "        training_num_loss = 0\n",
        "\n",
        "        # all_outputs = []\n",
        "        all_labels = []\n",
        "\n",
        "        classifier_results = {}\n",
        "        detector_results = {}\n",
        "        geolocation_results = {}\n",
        "        filepaths = []\n",
        "\n",
        "        # iterate through the dataloader batches. tqdm keeps track of progress.\n",
        "        for batch_n, batch in tqdm(\n",
        "            enumerate(train_dataloader), total=len(train_dataloader)\n",
        "        ):\n",
        "            \n",
        "            feature_map_images = batch['image']            \n",
        "\n",
        "            batch[\"image\"] = batch[\"image\"].permute(0, 2, 3, 1)     \n",
        "    \n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['label'].long().to(device)\n",
        "\n",
        "            image_ids = batch['image_id']\n",
        "\n",
        "            # 1) zero out the parameter gradients so that gradients from previous batches are not used in this step\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 2) run the foward step on this batch of images\n",
        "            logits = model(images)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "            # 3) compute the loss\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            training_loss += loss.item()\n",
        "            training_num_loss += 1\n",
        "\n",
        "            # let's keep track of the loss by epoch\n",
        "            tracking_loss[epoch] = loss.item()\n",
        "\n",
        "            # 4) compute our gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # update our weights\n",
        "            optimizer.step()\n",
        "\n",
        "            all_labels.append(labels.detach().cpu())\n",
        "\n",
        "            # storing information for ensembling method\n",
        "            for i in range(len(probs)):\n",
        "                # prob = pangolin_prob[i].item()\n",
        "                # label = \"pangolin\" if prob > 0.5 else \"other\"\n",
        "\n",
        "                image_id = image_ids[i]\n",
        "                filepath, _ = train_data[image_id]  # get filepath from index\n",
        "\n",
        "                filepaths.append(filepath)\n",
        "\n",
        "                class_probs = probs[i].cpu().tolist()\n",
        "\n",
        "                classifier_results[filepath] = {\n",
        "                    \"classifications\": {\n",
        "                        \"classes\": class_names,\n",
        "                        \"scores\": class_probs,\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                # detector result from batch\n",
        "                detector_result = batch[\"detections\"][i] if \"detections\" in batch else []\n",
        "                detector_results[filepath] = {\n",
        "                    \"detections\": detector_result\n",
        "                }\n",
        "\n",
        "        # print and log scores\n",
        "        training_loss /= training_num_loss\n",
        "        print(f\"\\ttraining loss: {training_loss:.4f}\")\n",
        "\n",
        "        all_labels = torch.cat(all_labels, dim=0)\n",
        "        train_true = all_labels             # get the true labels\n",
        "        binary_train_true = np.isin(train_true, pangolin_indices).astype(int)\n",
        "        \n",
        "        ensemble_outputs = speciesnet_ensembler.combine(\n",
        "            filepaths=filepaths,\n",
        "            classifier_results=classifier_results,\n",
        "            detector_results=detector_results,\n",
        "            geolocation_results=geolocation_results,\n",
        "            partial_predictions={},\n",
        "        )\n",
        "\n",
        "        ensemble_labels = []\n",
        "        ensemble_scores = []\n",
        "\n",
        "        for o in ensemble_outputs:\n",
        "            pred_class = o[\"prediction\"]\n",
        "            class_scores = o[\"prediction_score\"]\n",
        "            \n",
        "            if pred_class in pangolin_indices and o[\"prediction_score\"] >= 0.5:\n",
        "                ensemble_labels.append(1)  # pangolin\n",
        "            else:\n",
        "                ensemble_labels.append(0)  # other\n",
        "\n",
        "            ensemble_scores.append(o[\"prediction_score\"])\n",
        "\n",
        "\n",
        "        ensemble_labels = [1 if \"pangolin\" in o[\"prediction\"] else 0 for o in ensemble_outputs]\n",
        "        ensemble_scores = [o[\"prediction_score\"] for o in ensemble_outputs]\n",
        "\n",
        "        ensemble_pred = torch.tensor(ensemble_labels)\n",
        "        ensemble_prob = torch.tensor(ensemble_scores)\n",
        "\n",
        "        train_print_and_log_scores(fold, epoch, binary_train_true, ensemble_pred, ensemble_prob)\n",
        "\n",
        "        # outputting images for feature extraction\n",
        "        # num_images = 10\n",
        "        # random_indices = np.random.choice(len(feature_map_images), num_images, replace=False)\n",
        "\n",
        "        # mean = [0.485, 0.456, 0.406]\n",
        "        # std = [0.229, 0.224, 0.225]\n",
        "\n",
        "        # label_1_indices = [i for i in range(len(image_ids)) if train_data[image_ids[i]][1] == 1]\n",
        "        # label_0_indices = [i for i in range(len(image_ids)) if train_data[image_ids[i]][1] == 0]\n",
        "\n",
        "        # sampled_indices = random.sample(label_1_indices, 3) + random.sample(label_0_indices, 7)\n",
        "\n",
        "        # for img_idx in sampled_indices:\n",
        "        #     file_name, label = train_data[image_ids[img_idx]]\n",
        "        #     fig, axes = plt.subplots(1, len(layers_to_visualize) + 1, figsize=(30, 5))\n",
        "        #     fig.suptitle(f\"Label: {label}\", fontsize=20)\n",
        "\n",
        "        #     image = (feature_map_images[img_idx].cpu().numpy().transpose(1, 2, 0)) * std + mean\n",
        "        #     image = np.clip(image, 0, 1)\n",
        "\n",
        "        #     # image\n",
        "        #     axes[0].imshow(image, cmap='gray')\n",
        "        #     axes[0].set_title(\"original\")\n",
        "        #     axes[0].axis(\"off\")\n",
        "\n",
        "        #     for i, layer_name in enumerate(layers_to_visualize):\n",
        "        #         fmap = feature_maps[layer_name][img_idx]\n",
        "        #         fmap = fmap.mean(dim=0)\n",
        "        #         fmap = (fmap - fmap.min()) / (fmap.max() - fmap.min())\n",
        "\n",
        "        #         if fmap.ndimension() == 3:\n",
        "        #             num_channels = fmap.shape[0]\n",
        "        #             random_channel = np.random.randint(num_channels)\n",
        "        #             fmap_to_show = fmap[random_channel].cpu().numpy()\n",
        "        #         else:\n",
        "        #             random_channel = 0\n",
        "        #             fmap_to_show = fmap.cpu().numpy()\n",
        "\n",
        "        #         fmap_to_show = (fmap_to_show - fmap_to_show.min()) / (fmap_to_show.max() - fmap_to_show.min() + 1e-5)\n",
        "        #         axes[i + 1].imshow(fmap_to_show, cmap='viridis')\n",
        "\n",
        "        #         title = '/'.join(layer_name.split('/')[-2:])\n",
        "\n",
        "        #         axes[i + 1].set_title(f\"{title}\")\n",
        "        #         axes[i + 1].axis(\"off\")\n",
        "\n",
        "        #     feature_map_image_path  = feature_map_folder_path + \"feature_map_fold_\" + str(fold + 1) + \"/epoch_\" + str(epoch) + \"/\"      # path to store the feature map images, name-specific to fold and epoch \n",
        "        #     feature_map_image =  feature_map_image_path + str(image_index) + \".png\"\n",
        "\n",
        "        #     image_index += 1\n",
        "\n",
        "        #     os.makedirs(os.path.dirname(feature_map_image), exist_ok=True)\n",
        "        #     plt.savefig(feature_map_image, bbox_inches='tight')\n",
        "        #     plt.close()\n",
        "\n",
        "        #############################################################################################################################################\n",
        "        # validation\n",
        "        #############################################################################################################################################\n",
        "        print(\"VALIDATION\")\n",
        "\n",
        "        # val_preds_collector = []\n",
        "\n",
        "        val_loss = 0.0\n",
        "        val_num_loss = 0\n",
        "\n",
        "        classifier_results = {}\n",
        "        detector_results = {}\n",
        "        geolocation_results = {}\n",
        "        filepaths = []\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # iterate through dataloader and run the model\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_dataloader, total=len(val_dataloader)):\n",
        "                batch[\"image\"] = batch[\"image\"].permute(0, 2, 3, 1)     \n",
        "        \n",
        "                images = batch['image'].to(device)\n",
        "                labels = batch['label'].long().to(device)\n",
        "\n",
        "                image_ids = batch['image_id']\n",
        "                logits = model.forward(images)\n",
        "                \n",
        "                probs = torch.softmax(logits, dim=1)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_num_loss += 1\n",
        "\n",
        "                # storing information for ensembling method\n",
        "                for i in range(len(probs)):\n",
        "                    image_id = image_ids[i]\n",
        "                    filepath, _ = val_data[image_id]  # get filepath from index\n",
        "\n",
        "                    filepaths.append(filepath)\n",
        "\n",
        "                    class_probs = probs[i].cpu().tolist()\n",
        "\n",
        "                    classifier_results[filepath] = {\n",
        "                        \"classifications\": {\n",
        "                            \"classes\": class_names,\n",
        "                            \"scores\": class_probs,\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "                    # detector result from batch\n",
        "                    detector_result = batch[\"detections\"][i] if \"detections\" in batch else []\n",
        "                    detector_results[filepath] = {\n",
        "                        \"detections\": detector_result\n",
        "                    }\n",
        "\n",
        "        # print scores\n",
        "        val_loss /= val_num_loss\n",
        "        print(f\"\\tvalidation loss: {val_loss:.4f}\")\n",
        "\n",
        "        val_true = [label for _, label in val_data]          # get the true labels\n",
        "        binary_val_true = np.isin(val_true, pangolin_indices).astype(int)\n",
        "        \n",
        "        ensemble_outputs = speciesnet_ensembler.combine(\n",
        "            filepaths=filepaths,\n",
        "            classifier_results=classifier_results,\n",
        "            detector_results=detector_results,\n",
        "            geolocation_results=geolocation_results,\n",
        "            partial_predictions={},\n",
        "        )\n",
        "\n",
        "        ensemble_labels = []\n",
        "        ensemble_scores = []\n",
        "\n",
        "        for o in ensemble_outputs:\n",
        "            pred_class = o[\"prediction\"]\n",
        "            class_scores = o[\"prediction_score\"]\n",
        "            \n",
        "            if pred_class in pangolin_indices and o[\"prediction_score\"] >= 0.5:\n",
        "                ensemble_labels.append(1)  # pangolin\n",
        "            else:\n",
        "                ensemble_labels.append(0)  # other\n",
        "\n",
        "            ensemble_scores.append(o[\"prediction_score\"])\n",
        "\n",
        "        ensemble_labels = [1 if \"pangolin\" in o[\"prediction\"] else 0 for o in ensemble_outputs]\n",
        "        ensemble_scores = [o[\"prediction_score\"] for o in ensemble_outputs]\n",
        "\n",
        "        ensemble_pred = torch.tensor(ensemble_labels)\n",
        "        ensemble_prob = torch.tensor(ensemble_scores)\n",
        "\n",
        "        val_print_and_log_scores(fold, epoch, binary_val_true, ensemble_pred, ensemble_prob)\n",
        "\n",
        "        # log train and val loss\n",
        "        loss_writer.writerow([fold, epoch, training_loss, val_loss])\n",
        "\n",
        "        # check if the current epoch is most optimal based on the current minimum validation loss\n",
        "        if val_loss < min_val_loss:\n",
        "            # save model checkpoint to folder\n",
        "            model_path = os.path.join(model_folder_path, model_name)\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "\n",
        "            # update min validation loss value\n",
        "            min_val_loss = val_loss\n",
        "\n",
        "        # get and log the elapsed time for current epoch\n",
        "        epoch_time = time.time() - start_time\n",
        "        time_writer.writerow([fold, epoch, epoch_time])\n",
        "\n",
        "    #############################################################################################################################################\n",
        "    # testing\n",
        "    #############################################################################################################################################\n",
        "    print(\"TESTING\")\n",
        "\n",
        "    speciesnet_model = SpeciesNet(speciesnet_model_path)\n",
        "    base_model = speciesnet_model.classifier.model\n",
        "    base_model.fc = nn.Sequential(\n",
        "        nn.Linear(2048, 100),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(100, 2498),\n",
        "    )\n",
        "\n",
        "    base_model.load_state_dict(torch.load(model_path))\n",
        "    model = base_model.to(device)\n",
        "\n",
        "    # test_preds_collector = []\n",
        "\n",
        "    classifier_results = {}\n",
        "    detector_results = {}\n",
        "    geolocation_results = {}\n",
        "    filepaths = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # iterate through dataloader and run the model\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_dataloader, total=len(test_dataloader)):        \n",
        "            batch[\"image\"] = batch[\"image\"].permute(0, 2, 3, 1)      \n",
        "\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['label'].long().to(device)    \n",
        "            image_ids = batch['image_id']\n",
        "\n",
        "            logits = model.forward(images)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "            # storing information for ensembling method\n",
        "            for i in range(len(probs)):\n",
        "                image_id = image_ids[i]\n",
        "                filepath, _ = test_data[image_id]  # get filepath from index\n",
        "\n",
        "                filepaths.append(filepath)\n",
        "\n",
        "                class_probs = probs[i].cpu().tolist()\n",
        "\n",
        "                classifier_results[filepath] = {\n",
        "                    \"classifications\": {\n",
        "                        \"classes\": class_names,\n",
        "                        \"scores\": class_probs,\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                # detector result from batch\n",
        "                detector_result = batch[\"detections\"][i] if \"detections\" in batch else []\n",
        "                detector_results[filepath] = {\n",
        "                    \"detections\": detector_result\n",
        "                }\n",
        "\n",
        "    # print and log scores\n",
        "    test_true = [label for _, label in test_data]              # get the true labels\n",
        "    binary_test_true = np.isin(test_true, pangolin_indices).astype(int)\n",
        "    \n",
        "    ensemble_outputs = speciesnet_ensembler.combine(\n",
        "        filepaths=filepaths,\n",
        "        classifier_results=classifier_results,\n",
        "        detector_results=detector_results,\n",
        "        geolocation_results=geolocation_results,\n",
        "        partial_predictions={},\n",
        "    )\n",
        "\n",
        "    ensemble_labels = []\n",
        "    ensemble_scores = []\n",
        "\n",
        "    for o in ensemble_outputs:\n",
        "        pred_class = o[\"prediction\"]\n",
        "        class_scores = o[\"prediction_score\"]\n",
        "        \n",
        "        if pred_class in pangolin_indices and o[\"prediction_score\"] >= 0.5:\n",
        "            ensemble_labels.append(1)  # pangolin\n",
        "        else:\n",
        "            ensemble_labels.append(0)  # other\n",
        "\n",
        "        ensemble_scores.append(o[\"prediction_score\"])\n",
        "\n",
        "    ensemble_labels = [1 if \"pangolin\" in o[\"prediction\"] else 0 for o in ensemble_outputs]\n",
        "    ensemble_scores = [o[\"prediction_score\"] for o in ensemble_outputs]\n",
        "\n",
        "    ensemble_pred = torch.tensor(ensemble_labels)\n",
        "    ensemble_prob = torch.tensor(ensemble_scores)\n",
        "\n",
        "    test_print_and_log_scores(fold, binary_test_true, ensemble_pred, ensemble_prob)\n",
        "\n",
        "    # error analysis on misclassified images\n",
        "    plot_misclassified(fold, ensemble_pred, test_data)\n",
        "\n",
        "    # display confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    plt.title(\"confusion matrix - fold \" + str(fold + 1))\n",
        "    cm = ConfusionMatrixDisplay.from_predictions(\n",
        "        test_true,\n",
        "        ensemble_pred,\n",
        "        ax=ax,\n",
        "        xticks_rotation=90,\n",
        "        colorbar=True,\n",
        "        normalize='true'\n",
        "    )\n",
        "\n",
        "    cm_image_path  = confusion_matrix_folder_path      # path to store the confusion matrix, name-specific to fold \n",
        "    cm_image = cm_image_path + \"cm_fold_\" + str(fold + 1) + \".png\"\n",
        "\n",
        "    os.makedirs(os.path.dirname(cm_image), exist_ok=True)\n",
        "    plt.savefig(cm_image, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    # flush writes to logging files\n",
        "    train_csv.flush()\n",
        "    time_csv.flush()\n",
        "    test_csv.flush()\n",
        "    loss_csv.flush()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG5oysIYGjQF"
      },
      "outputs": [],
      "source": [
        "train_csv.flush()\n",
        "val_csv.flush()\n",
        "time_csv.flush()\n",
        "test_csv.flush()\n",
        "loss_csv.flush()\n",
        "\n",
        "train_csv.close()\n",
        "val_csv.close()\n",
        "time_csv.close()\n",
        "test_csv.close()\n",
        "loss_csv.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
